{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16f33ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "class SoftmaxWeight(nn.Module):\n",
    "    def __init__(self, K, p, hidden_dimensions =[]):\n",
    "        super().__init__()\n",
    "        self.K = K\n",
    "        self.p = p\n",
    "        self.network_dimensions = [self.p] + hidden_dimensions + [self.K]\n",
    "        network = []\n",
    "        for h0, h1 in zip(self.network_dimensions, self.network_dimensions[1:]):\n",
    "            network.extend([nn.Linear(h0, h1),nn.Tanh(),])\n",
    "        network.pop()\n",
    "        self.f = nn.Sequential(*network)\n",
    "        self.f = nn.Sequential(*network)\n",
    "        self.f[-1].bias = nn.Parameter(torch.ones(self.K))\n",
    "        self.f[-1].weight = nn.Parameter(torch.zeros_like(self.f[-1].weight))\n",
    "\n",
    "    def log_prob(self, z):\n",
    "        unormalized_log_w = self.f.forward(z)\n",
    "        return unormalized_log_w - torch.logsumexp(unormalized_log_w, dim=-1, keepdim=True)\n",
    "    \n",
    "class ConvNetWeight(nn.Module):\n",
    "    def __init__(self, K):\n",
    "        super(ConvNetWeight, self).__init__()\n",
    "        self.K = K\n",
    "        self.conv1 = nn.Conv2d(3, 8, 3, 1)\n",
    "        #premier argument = couleur vs gris 3 vs 1\n",
    "        self.conv2 = nn.Conv2d(8, 8, 3, 1)\n",
    "        self.dropout1 = nn.Dropout2d(0.25)\n",
    "        self.dropout2 = nn.Dropout2d(0.5)\n",
    "        self.fc1 = nn.Linear(14*14*8, 128)\n",
    "        self.fc2 = nn.Linear(128, K)\n",
    "\n",
    "        # x represents our data\n",
    "    def forward(self, x):\n",
    "        # Pass data through conv1\n",
    "        x = self.conv1(x)\n",
    "        # Use the rectified-linear activation function over x\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        # Run max pooling over x\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        # Pass data through dropout1\n",
    "        x = self.dropout1(x)\n",
    "        # Flatten x with start_dim=1\n",
    "        x = torch.flatten(x, 1)\n",
    "        # Pass data through fc1\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "    def log_prob(self, z):\n",
    "        unormalized_log_w = self.forward(z)\n",
    "        return unormalized_log_w - torch.logsumexp(unormalized_log_w, dim=-1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "766c010b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4eec4ef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "###MNIST###\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "import matplotlib.pyplot as plt\n",
    "fmnist_trainset = datasets.CIFAR10(root='./data', train=True, download=True, transform=None)\n",
    "fmnist_testset = datasets.CIFAR10(root='./data', train=False, download=True, transform=None)\n",
    "randperm_train = torch.randperm(torch.tensor(fmnist_trainset.targets).shape[0])\n",
    "randperm_test_val = torch.randperm(torch.tensor(fmnist_testset.targets).shape[0])\n",
    "\n",
    "num_train = 5000\n",
    "\n",
    "train_labels = torch.tensor(fmnist_trainset.targets)[randperm_train][:num_train]\n",
    "test_labels = torch.tensor(fmnist_testset.targets)[randperm_test_val][:5000]\n",
    "val_labels = torch.tensor(fmnist_testset.targets)[randperm_test_val][5000:]\n",
    "\n",
    "extracted_train = torch.tensor(fmnist_trainset.data).float()[randperm_train][:num_train].reshape(num_train,3,32,32)\n",
    "train_samples = (extracted_train + torch.rand(extracted_train.shape))/256\n",
    "extracted_test = torch.tensor(fmnist_testset.data).float()[randperm_test_val][:5000].reshape(5000,3,32,32)\n",
    "test_samples = (extracted_test + torch.rand(extracted_test.shape))/256\n",
    "extracted_val = torch.tensor(fmnist_testset.data).float()[randperm_test_val][5000:].reshape(5000,3,32,32)\n",
    "val_samples = (extracted_val + torch.rand(extracted_val.shape))/256\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d4fad6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f441c479",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [27:08<00:00,  1.63s/it, loss_train = 0.1407; acc_train =0.3994; acc_test =0.2048; acc_validation =0.2132]\n"
     ]
    }
   ],
   "source": [
    "w = SoftmaxWeight(10,train_samples.shape[-1],[128,128,128])\n",
    "w = ConvNetWeight(10)\n",
    "optim = torch.optim.Adam(w.parameters(), lr = 5e-4)\n",
    "list_accuracy = []\n",
    "device = torch.device('cpu' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_samples = train_samples.to(device)\n",
    "train_labels = train_labels.to(device)\n",
    "\n",
    "test_samples = test_samples.to(device)\n",
    "test_labels = test_labels.to(device)\n",
    "\n",
    "val_samples = val_samples.to(device)\n",
    "val_labels = val_labels.to(device)\n",
    "\n",
    "counts = torch.unique(train_labels, return_counts = True)[1]/train_labels.shape[0]\n",
    "weights = torch.distributions.Dirichlet(torch.ones(train_labels.shape[0])).sample().to(device)\n",
    "list_accuracy_train = []\n",
    "list_accuracy_test = []\n",
    "list_accuracy_val = []\n",
    "\n",
    "w.to(device)\n",
    "pbar = tqdm(range(1000))\n",
    "for t in pbar:\n",
    "    optim.zero_grad()\n",
    "    loss_train = -torch.sum(weights*(w.log_prob(train_samples)*counts)[range(train_samples.shape[0]),train_labels])\n",
    "    with torch.no_grad():\n",
    "        accuracy_train = torch.mean((torch.max(w.log_prob(train_samples), dim = 1)[1] == train_labels).float())\n",
    "        list_accuracy_train.append(accuracy_train.cpu().item())\n",
    "        accuracy_test = torch.mean((torch.max(w.log_prob(test_samples), dim = 1)[1] == test_labels).float())\n",
    "        list_accuracy_test.append(accuracy_test.cpu().item())\n",
    "        accuracy_val= torch.mean((torch.max(w.log_prob(val_samples), dim = 1)[1] == val_labels).float())\n",
    "        list_accuracy_val.append(accuracy_val.cpu().item())\n",
    "    loss_train.backward()\n",
    "    optim.step()\n",
    "    pbar.set_postfix_str('loss_train = ' + str(round(loss_train.item(),4)) +'; acc_train =' + str(round(accuracy_train.item(),4)) + '; acc_test =' + str(round(accuracy_test.item(),4)) + '; acc_validation =' + str(round(accuracy_val.item(),4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e941fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (15,8))\n",
    "plt.plot(list_accuracy_train, label = 'train')\n",
    "plt.plot(list_accuracy_test, label = 'test')\n",
    "plt.plot(list_accuracy_val, label = 'validation')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aaf22d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_accuracy_current_gibbs = []\n",
    "list_accuracy_train_gibbs = []\n",
    "list_accuracy_test_gibbs = []\n",
    "list_accuracy_val_gibbs = []\n",
    "pbar = tqdm(range(100))\n",
    "log_prob_train_gibbs = []\n",
    "log_prob_test_gibbs = []\n",
    "log_prob_val_gibbs = []\n",
    "\n",
    "w = SoftmaxWeight(10,train_samples.shape[-1],[128,128,128]).to(device)\n",
    "current_samples = torch.cat([train_samples.to(device), test_samples.to(device)], dim = 0)\n",
    "for i in pbar:\n",
    "    fake_labels= torch.distributions.Categorical(torch.exp(w.to(device).log_prob(test_samples.to(device)))).sample()\n",
    "    current_labels = torch.cat([train_labels.to(device), fake_labels], dim =0)\n",
    "    counts = torch.unique(current_labels, return_counts = True)[1]/current_labels.shape[0]\n",
    "    weights = torch.distributions.Dirichlet(torch.ones(current_labels.shape[0])).sample().to(device)\n",
    "    w = SoftmaxWeight(10,train_samples.shape[-1],[128,128,128]).to(device)\n",
    "    optim = torch.optim.Adam(w.parameters(), lr = 5e-4)\n",
    "    for t in range(500):\n",
    "        optim.zero_grad()\n",
    "        loss_train = -torch.sum(weights*(w.log_prob(current_samples))[torch.tensor(range(current_labels.shape[0])).to(device),current_labels])\n",
    "        with torch.no_grad():\n",
    "            accuracy_current = torch.mean((torch.max(w.log_prob(current_samples), dim = 1)[1] == current_labels).float()).cpu()\n",
    "            accuracy_val = torch.mean((torch.max(w.log_prob(val_samples), dim = 1)[1] == val_labels).float()).cpu()\n",
    "            accuracy_train = torch.mean((torch.max(w.log_prob(train_samples), dim = 1)[1] == train_labels).float()).cpu()\n",
    "            accuracy_test = torch.mean((torch.max(w.log_prob(test_samples), dim = 1)[1] == test_labels).float()).cpu()\n",
    "            list_accuracy_current_gibbs.append(accuracy_current.item())\n",
    "            list_accuracy_train_gibbs.append(accuracy_train.item())\n",
    "            list_accuracy_test_gibbs.append(accuracy_test.item())\n",
    "            list_accuracy_val_gibbs.append(accuracy_val.item())\n",
    "        loss_train.backward()\n",
    "        optim.step()\n",
    "        pbar.set_postfix_str('loss_train = ' + str(round(loss_train.item(),4))+'; acc_current =' + str(round(accuracy_current.item(),4)) +'; acc_train =' + str(round(accuracy_train.item(),4)) + '; acc_test =' + str(round(accuracy_test.item(),4)) + '; acc_val =' + str(round(accuracy_val.item(),4)))\n",
    "\n",
    "    clear_output(wait = True)\n",
    "    plt.hist(torch.prod(torch.exp(w.log_prob(test_samples)), dim = -1).cpu().detach().numpy(), bins = 50)\n",
    "    plt.show()\n",
    "    clear_output(wait = True)\n",
    "    plt.figure(figsize = (15,8))\n",
    "    plt.plot(list_accuracy_train_gibbs, label = 'Train')\n",
    "    plt.plot(list_accuracy_current_gibbs, label ='Current')\n",
    "    plt.plot(list_accuracy_val_gibbs, label = 'Validation')\n",
    "    plt.plot(list_accuracy_test_gibbs, label = 'Test')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    log_prob_train_gibbs.append(w.log_prob(train_samples))\n",
    "    log_prob_test_gibbs.append(w.log_prob(test_samples))\n",
    "    log_prob_val_gibbs.append(w.log_prob(val_samples))\n",
    "bagging_log_prob_train_gibbs = torch.mean(torch.stack(log_prob_train_gibbs), dim =0)\n",
    "bagging_log_prob_test_gibbs = torch.mean(torch.stack(log_prob_test_gibbs), dim =0)\n",
    "bagging_log_prob_val_gibbs = torch.mean(torch.stack(log_prob_val_gibbs), dim =0)\n",
    "bagging_accuracy_train= torch.mean((torch.max(bagging_log_prob_train_gibbs, dim = 1)[1] == train_labels).float())\n",
    "print(bagging_accuracy_train)\n",
    "bagging_accuracy_test = torch.mean((torch.max(bagging_log_prob_test_gibbs, dim = 1)[1] == test_labels).float())\n",
    "print(bagging_accuracy_test)\n",
    "bagging_accuracy_val= torch.mean((torch.max(bagging_log_prob_val_gibbs, dim = 1)[1] == val_labels).float())\n",
    "print(bagging_accuracy_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca827e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
